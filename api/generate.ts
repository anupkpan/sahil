import type { VercelRequest, VercelResponse } from "@vercel/node";
import { OpenAI } from "openai";
import { loadCachedShayaris } from "../eknazariyaScraper";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! });

export default async function handler(req: VercelRequest, res: VercelResponse) {
  if (req.method !== "POST") {
    return res.status(405).json({ error: "Only POST requests allowed" });
  }

  const { mood = "", theme = "", depth = 5 } = req.body;

  try {
    console.log("üîÑ (Re)Loading shayari cache...");
    const shayariCache = await loadCachedShayaris();

    const matches = shayariCache.filter(
      (s) => s.mood.includes(mood) && s.theme.includes(theme)
    );

    if (matches.length > 0) {
      const lines = matches[Math.floor(Math.random() * matches.length)].lines;
      return res.status(200).json({ lines, source: "eknazariya.com" });
    }

    // Fallback to OpenAI
    const prompt = `‡§è‡§ï ${mood || "‡§≠‡§æ‡§µ‡•Å‡§ï"} ‡§î‡§∞ ${theme || "‡§™‡•ç‡§∞‡•á‡§Æ"} ‡§µ‡§ø‡§∑‡§Ø ‡§™‡§∞ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ${
      depth > 7 ? "‡§ó‡§π‡§∞‡•Ä" : "‡§∏‡§∞‡§≤"
    } ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§∂‡§æ‡§Ø‡§∞‡•Ä ‡§¨‡§§‡§æ‡§ì‡•§`;

    const result = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: [
        {
          role: "system",
          content: "‡§§‡•Å‡§Æ ‡§è‡§ï ‡§≠‡§æ‡§µ‡•Å‡§ï ‡§â‡§∞‡•ç‡§¶‡•Ç-‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§∂‡§æ‡§Ø‡§∞ ‡§π‡•ã‡•§ ‡§ï‡•á‡§µ‡§≤ ‡§∂‡§æ‡§Ø‡§∞‡•Ä ‡§¶‡•ã‡•§",
        },
        { role: "user", content: prompt },
      ],
    });

    const text = result.choices[0].message.content || "";
    const lines = text
      .split("\n")
      .map((line) => line.trim())
      .filter((line) => line.length > 0);

    return res.status(200).json({ lines, source: "OpenAI" });
  } catch (err: any) {
    console.error("‚ùå API error", err);
    return res
      .status(500)
      .json({ error: err.message || "‡§∂‡§æ‡§Ø‡§∞‡•Ä ‡§¨‡§®‡§æ‡§®‡•á ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§π‡•Å‡§à‡•§" });
  }
}
